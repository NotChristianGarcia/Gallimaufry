{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fec340-9a3f-45d7-bce7-a2202ac338c8",
   "metadata": {},
   "source": [
    "## langchain\n",
    "Some examples/messing around with langchain. There's three main headers, run the initial pip3 install/import located below this and then run code under headers in order.  \n",
    "Ensure you fill in the OPENAI_API_KEY variable in the cells below. It's used by the OpenAI() function. Environment variable makes it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a314148-c95b-4e26-b2c1-a62c430ee466",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain openai beautifulsoup4 tiktoken faiss-cpu chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2207425-3de3-4127-a750-997a1ac0287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['OPENAI_API_KEY'] = \"Your key here, get from here, they give you an initial amount of credit for free https://platform.openai.com/account/api-keys\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec91437-9ab9-46df-b4dd-669e8f09cdfa",
   "metadata": {},
   "source": [
    "## Just OpenAI\n",
    "Loads the a chat object that allows users to send messages and receive messages from the API. Context is saved per message allowing the LLM to use previous messages as context. Giving history when using API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec1cf127-9672-4833-9224-f5ad0bec327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  dsa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't understand what you mean by \"dsa\". Can you please provide more context or information?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I still don't understand what you mean by \"d\". Can you please provide more context or information?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  What did I just write?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You wrote \"d\".\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "messages = []\n",
    "\n",
    "i = 0\n",
    "while i < 3:\n",
    "    message = input(\"> \")\n",
    "    usr_msg = HumanMessage(content=message)\n",
    "    messages.append(usr_msg)\n",
    "    ai_msg = chat(messages)\n",
    "    print(ai_msg.content)\n",
    "    messages.append(ai_msg)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974f2a4-0b4a-4a58-9448-2c274a857df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc392b3c-12cc-4195-bde8-7eb1a119f886",
   "metadata": {},
   "source": [
    "## Read the Docs\n",
    "Download RtD site with wget. Save to folder. Then use the RtD loader, parsing said information, splitting text, using chroma for vector storage, and then running Q/A against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803913b5-9962-4476-b8d5-cf2ffdecb18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -r -A.html -P rtdocs https://tapis.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adaa91eb-3288-4c53-948a-4a220e063e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ReadTheDocsLoader(\"rtdocs\", features='html.parser', errors='ignore')\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    "    )\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7c4571-c7c2-4e2c-9ac4-5f626249122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes. You can use the following code example to create a pod for neo4j using tapipy. It is important to note that the tapipy library should be version 1.2.3 or higher to use the pods service:\n",
      "\n",
      "# Import the Tapis object\n",
      "from tapipy.tapis import Tapis\n",
      "# Log into you the Tapis service by providing user/pass and url.\n",
      "t = Tapis(base_url='https://tacc.tapis.io',\n",
      "          username='your username',\n",
      "          password='your password')\n",
      "# Get tokens that will be used for authenticated function calls\n",
      "t.get_tokens()\n",
      "\n",
      "# To register an pod using the tapipy library, we use the pods.create_pod() method and pass the arguments describing\n",
      "# the pod we want to register through the function parameters. For example:\n",
      "t.pods.create_pod(pod_id='docpod', pod_template='neo4j', description='My example pod!')\n"
     ]
    }
   ],
   "source": [
    "#query = \"Can you give me a code example of using tapipy and creating an abaco actor?\"\n",
    "query = \"Can you give me a code example of using tapipy and creating a pod for neo4j?\"\n",
    "#query = \"Can you give me a code example of using tapipy and creating a tapis system and uploading a file to it?\"\n",
    "\n",
    "res = qa.run(query)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5791d98-e072-4e70-be02-b6cf3cea7490",
   "metadata": {},
   "source": [
    "# OpenAPI with Planners\n",
    "Downloading the pgrest openapi schema and using an openapi agent to plan a workflow of requests based on a user query.  \n",
    "It would be great to have multiple schemas readable at once, don't quite know how to do that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4009c7b9-df13-4317-a70b-7289c8f10550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-25 11:44:47--  https://raw.githubusercontent.com/tapis-project/tapipy/main/tapipy/resources/openapi_v3-pgrest.yml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33814 (33K) [text/plain]\n",
      "Saving to: ‘openapi_v3-pgrest.yml.1’\n",
      "\n",
      "openapi_v3-pgrest.y 100%[===================>]  33.02K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2023-05-25 11:44:48 (7.41 MB/s) - ‘openapi_v3-pgrest.yml.1’ saved [33814/33814]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/tapis-project/tapipy/main/tapipy/resources/openapi_v3-pgrest.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55f6267e-83a6-43e9-b2d7-d1eec664e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "from langchain.agents.agent_toolkits.openapi.spec import reduce_openapi_spec\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents.agent_toolkits.openapi import planner\n",
    "from langchain.requests import RequestsWrapper\n",
    "\n",
    "with open(\"openapi_v3-pgrest.yml\") as f:\n",
    "    raw_openai_api_spec = yaml.load(f, Loader=yaml.Loader)\n",
    "openai_api_spec = reduce_openapi_spec(raw_openai_api_spec)\n",
    "\n",
    "#This is where you can set headers if you want\n",
    "headers = {}\n",
    "requests_wrapper = RequestsWrapper(headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0813c39-9d73-47e3-99a0-88be9024f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.0)\n",
    "pgrest_agent = planner.create_openapi_agent(openai_api_spec, requests_wrapper, llm)\n",
    "\n",
    "user_query = \"Use pgrest to create a new role named MYROLE, create a new table, and add a row to the table, and create a view of the table restricted to MYROLE. Don't actually run the requests, only plan\"\n",
    "pgrest_agent.run(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b657a-7369-4853-92da-7835f4cb48fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89b93f21-68c8-4340-84a7-ce4c9bf87570",
   "metadata": {},
   "source": [
    "## Combining multiple toolkits/vectorstores\n",
    "I'd like to be able to combine multiple toolkits (more than one create_openapi_agent for example). Logically I doubt this makes sense, but there's an issue.\n",
    "- https://github.com/hwchase17/langchain/issues/3435\n",
    "\n",
    "## Docs\n",
    "Just some useful doc links for additional context. Docs have a decent amount of examples that prove useful.\n",
    "- https://python.langchain.com/en/latest/reference/modules/agents.html?highlight=create_openapi_agent#langchain.agents.create_openapi_agent\n",
    "- https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html#prepare-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fd04b8c-be7f-4520-b00d-bb4eb75a2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mainly stolen - https://github.com/hwchase17/chat-langchain\n",
    "# This is using FAISS instead of chroma for the vectorstore. Couldn't quite figure out how to use said vectorstore though. \n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "loader = ReadTheDocsLoader(\"rtdoc\", features='html.parser')\n",
    "raw_documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Save vectorstore\n",
    "with open(\"vectorstore.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
