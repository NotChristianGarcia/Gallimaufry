{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aloe Token\n",
    "token=\"74b1aaf8ca3f131071be811964fd2987\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abaco Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import requests as r\n",
    "\n",
    "header_dat={'Authorization': f'Bearer {token}'}\n",
    "url = \"https://dev.tenants.aloedev.tacc.cloud/actors/v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Actors retrieved successfully.',\n",
       " 'result': [{'_links': {'executions': 'http://172.17.0.1:8000/actors/v2/Lv6klXBzGNkEJ/executions',\n",
       "    'owner': 'http://172.17.0.1:8000/profiles/v2/testuser8',\n",
       "    'self': 'http://172.17.0.1:8000/actors/v2/Lv6klXBzGNkEJ'},\n",
       "   'createTime': '2019-06-13 18:15:53.777929',\n",
       "   'defaultEnvironment': {},\n",
       "   'description': '',\n",
       "   'id': 'Lv6klXBzGNkEJ',\n",
       "   'image': 'notchristiangarcia/flops_test:test-1.0',\n",
       "   'lastUpdateTime': '2019-06-13 18:15:53.777929',\n",
       "   'mounts': [{'container_path': '/_abaco_data1',\n",
       "     'host_path': '/home/apim/data1',\n",
       "     'mode': 'ro'},\n",
       "    {'container_path': '/_abaco_data2',\n",
       "     'host_path': '/home/apim/data2',\n",
       "     'mode': 'rw'}],\n",
       "   'owner': 'testuser8',\n",
       "   'privileged': False,\n",
       "   'queue': 'default',\n",
       "   'state': {},\n",
       "   'stateless': True,\n",
       "   'status': 'READY',\n",
       "   'statusMessage': ' ',\n",
       "   'type': 'none',\n",
       "   'useContainerUid': False}],\n",
       " 'status': 'success',\n",
       " 'version': ':dev'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all actors.\n",
    "actor = r.get(f\"{url}\",\n",
    "               headers=header_dat)\n",
    "actor.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create actor for work\n",
    "actor = r.post(f\"{url}\",\n",
    "               headers=header_dat,\n",
    "               data={'image':'notchristiangarcia/flops_test:test-1.0'})\n",
    "actor_id = actor.json()['result']['id']\n",
    "actor.json();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Actor deleted successfully.', 'result': None, 'status': 'success', 'version': ':dev'}\n",
      "{'message': 'Actor deleted successfully.', 'result': None, 'status': 'success', 'version': ':dev'}\n"
     ]
    }
   ],
   "source": [
    "# Delete specified actors.\n",
    "for del_actor in ['Y66xBXgwEYA5D' ,'RrOGMZpo5Az5o']:\n",
    "    del_actor_res = r.delete(f\"{url}/{del_actor}\",\n",
    "                     headers=header_dat)\n",
    "    print(del_actor_res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Host 9': 1, 'Host 12': 1, 'Host 18': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of workers currently available\n",
    "worker_res = r.get(f\"{url}/{actor_id}/workers\",\n",
    "                    headers=header_dat)\n",
    "num_workers = len(worker_res.json()['result'])\n",
    "print(num_workers)\n",
    "\n",
    "host_info = {}\n",
    "all_workers = []\n",
    "try:\n",
    "    if num_workers:\n",
    "        for worker_info in worker_res.json()['result']:\n",
    "            if f\"Host {worker_info['hostId']}\" in host_info:\n",
    "                host_info[f\"Host {worker_info['hostId']}\"] += 1\n",
    "            else:\n",
    "                host_info[f\"Host {worker_info['hostId']}\"] = 1\n",
    "        for worker_info in worker_res.json()['result']:\n",
    "            all_workers.append(worker_info['id'])\n",
    "except KeyError:\n",
    "    print('Still spooling.')\n",
    "worker_res.json();\n",
    "host_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Scheduled 3 new worker(s) to start. Previously, there were 0 workers.',\n",
       " 'result': None,\n",
       " 'status': 'success',\n",
       " 'version': ':dev'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spool up more workers\n",
    "worker_res = r.post(f\"{url}/{actor_id}/workers\",\n",
    "                    headers=header_dat,\n",
    "                    data={'num': 3})\n",
    "worker_res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Workers:\n",
      "."
     ]
    }
   ],
   "source": [
    "# Delete all current workers!\n",
    "worker_res = r.get(f\"{url}/{actor_id}/workers\",\n",
    "                        headers=header_dat)\n",
    "print('Deleted Workers:')\n",
    "for worker_info in worker_res.json()['result']:\n",
    "    del_worker_res = r.delete(f\"{url}/{actor_id}/workers/{worker_info['id']}\",\n",
    "                        headers=header_dat)\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything that you really just don't need to see at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 2 #os.environ['nodes']\n",
    "\n",
    "# Get all current workers\n",
    "worker_res = r.get(f\"{url}/{actor_id}/workers\",\n",
    "                        headers=header_dat)\n",
    "all_workers = []\n",
    "for worker_info in worker_res.json()['result']:\n",
    "    all_workers.append(worker_info['id'])\n",
    "    \n",
    "# Delete all current workers\n",
    "for del_worker in all_workers:\n",
    "    del_worker_res = r.delete(f\"{url}/{actor_id}/workers/{del_worker}\",\n",
    "                        headers=header_dat)\n",
    "\n",
    "# Spool up more workers!\n",
    "worker_res = r.post(f\"{url}/{actor_id}/workers\",\n",
    "                    headers=header_dat,\n",
    "                    data={'num': nodes*6})\n",
    "\n",
    "# Check on amount of workers\n",
    "while True:\n",
    "    worker_res = r.get(f\"{url}/{actor_id}/workers\",\n",
    "                        headers=header_dat)\n",
    "    num_workers = len(worker_res.json()['result'])\n",
    "    if not num_workers == nodes*6:\n",
    "        continue\n",
    "    \n",
    "    host_info = {}\n",
    "    if num_workers:\n",
    "        try:\n",
    "            for worker_info in worker_res.json()['result']:\n",
    "                worker_info['hostId']\n",
    "            break\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame()\n",
    "\n",
    "actor_id = 'peorL6Eg4GPW3'\n",
    "\n",
    "nodes = 1\n",
    "workers_per_node = 6\n",
    "messages_per_worker = 10\n",
    "\n",
    "threads = 0        # set to 0 for all threads\n",
    "std_dev = 1000\n",
    "size = 1000\n",
    "executions = nodes * workers_per_node * messages_per_worker\n",
    "messageDat = f'{threads} {std_dev} {size}'\n",
    "runs = 3\n",
    "\n",
    "for run_num in range(1, runs + 1):\n",
    "    print(f'Starting Run {run_num}.')\n",
    "    msg_start = time.time()\n",
    "    exec_id_list = []\n",
    "    for i in range(executions):\n",
    "        execution = r.post(f\"{url}/{actor_id}/messages\",\n",
    "                           headers=header_dat,\n",
    "                           data={'message':messageDat})\n",
    "        exec_id = execution.json()['result']['executionId']\n",
    "        exec_id_list.append(exec_id)\n",
    "        print(f'\\rCreated execution: {i+1}', end='')\n",
    "    msg_end = time.time()\n",
    "\n",
    "    exec_start = time.time()\n",
    "    results_list = []\n",
    "    \n",
    "    executions_completed = 0\n",
    "    while exec_id_list:\n",
    "        for exec_id in exec_id_list:\n",
    "            exec_logs = r.get(f\"{url}/{actor_id}/executions/{exec_id}/logs\",\n",
    "                              headers=header_dat)\n",
    "\n",
    "            logs = exec_logs.json()['result']['logs']\n",
    "            if logs:\n",
    "                print(f'\\rExecutions completed: {executions_completed + 1}', end='')\n",
    "                results_list.append([f'id - {exec_id}', float(logs.replace('\\n',''))])\n",
    "                exec_id_list.remove(exec_id)\n",
    "                executions_completed += 1\n",
    "    exec_end = time.time()\n",
    "\n",
    "    work_time = 0\n",
    "    for res in results_list:\n",
    "        work_time += res[1]\n",
    "\n",
    "    print(f\"\\n\\nRun Number {run_num}\")\n",
    "    if threads:\n",
    "        print(f\"Threads: {threads}\")\n",
    "    print(f\"Std Dev: {std_dev}\")\n",
    "    print(f\"Size: {size}\")\n",
    "    print(f\"Executions: {executions}\")\n",
    "    print(f\"Message Time: {msg_end - msg_start}\")\n",
    "    print(f\"Exec Time: {exec_end - exec_start}\")\n",
    "    print(f\"Work Time: {work_time}\")\n",
    "    print('\\n')\n",
    "\n",
    "    run_data = pd.DataFrame([[run_num, threads, std_dev, size, executions, msg_end - msg_start, exec_end - exec_start, work_time]],\n",
    "                            columns=['Run Number', 'Threads', 'Std Dev', 'Size', 'Executions', 'Message Time', 'Exec Time', 'Work Time'])\n",
    "    all_data = all_data.append(run_data, ignore_index = True)\n",
    "all_data.to_csv(f'{nodes}_nodes_{runs}_trials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper paramater tester\n",
    "actor_id = 'peorL6Eg4GPW3'\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "for test_var in [1000, 2000, 3000]:\n",
    "    nodes = 1\n",
    "    workers_per_node = 6\n",
    "    messages_per_worker = 5\n",
    "\n",
    "    threads = 0        # set to 0 for all threads\n",
    "    std_dev = 1000\n",
    "    size = test_var\n",
    "    executions = nodes * workers_per_node * messages_per_worker\n",
    "    messageDat = f'{threads} {std_dev} {size}'\n",
    "    runs = 3\n",
    "\n",
    "    for run_num in range(1, runs + 1):\n",
    "        print(f'Starting Run {run_num}.')\n",
    "        msg_start = time.time()\n",
    "        exec_id_list = []\n",
    "        for i in range(executions):\n",
    "            execution = r.post(f\"{url}/{actor_id}/messages\",\n",
    "                               headers=header_dat,\n",
    "                               data={'message':messageDat})\n",
    "            exec_id = execution.json()['result']['executionId']\n",
    "            exec_id_list.append(exec_id)\n",
    "            print(f'\\rCreated execution: {i+1}', end='')\n",
    "        msg_end = time.time()\n",
    "\n",
    "        exec_start = time.time()\n",
    "        results_list = []\n",
    "\n",
    "        executions_completed = 0\n",
    "        while exec_id_list:\n",
    "            for exec_id in exec_id_list:\n",
    "                exec_logs = r.get(f\"{url}/{actor_id}/executions/{exec_id}/logs\",\n",
    "                                  headers=header_dat)\n",
    "\n",
    "                logs = exec_logs.json()['result']['logs']\n",
    "                if logs:\n",
    "                    print(f'\\rExecutions completed: {executions_completed + 1}', end='')\n",
    "                    results_list.append([f'id - {exec_id}', float(logs.replace('\\n',''))])\n",
    "                    exec_id_list.remove(exec_id)\n",
    "                    executions_completed += 1\n",
    "        exec_end = time.time()\n",
    "\n",
    "        work_time = 0\n",
    "        for res in results_list:\n",
    "            work_time += res[1]\n",
    "\n",
    "        print(f\"\\n\\nRun Number {run_num}\")\n",
    "        if threads:\n",
    "            print(f\"Threads: {threads}\")\n",
    "        print(f\"Std Dev: {std_dev}\")\n",
    "        print(f\"Size: {size}\")\n",
    "        print(f\"Executions: {executions}\")\n",
    "        print(f\"Message Time: {msg_end - msg_start}\")\n",
    "        print(f\"Exec Time: {exec_end - exec_start}\")\n",
    "        print(f\"Work Time: {work_time}\")\n",
    "        print('\\n')\n",
    "\n",
    "    run_data = pd.DataFrame([[run_num, threads, std_dev, size, executions, msg_end - msg_start, exec_end - exec_start, work_time]],\n",
    "                            columns=['Run Number', 'Threads', 'Std Dev', 'Size', 'Executions', 'Message Time', 'Exec Time', 'Work Time'])\n",
    "    all_data = all_data.append(run_data, ignore_index = True)\n",
    "all_data.to_csv(f'{nodes}_nodes_{runs}_trials.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = \"'std_deviation' 'size'\"\n",
    "messageDat = '3 10 1000'\n",
    "executions = 8\n",
    "\n",
    "# Actor that will be creating workers with flop test.\n",
    "actor = r.post(f\"{url}/actors\",\n",
    "               headers=header_dat,\n",
    "               data={'image':'notchristiangarcia/flops_test'})\n",
    "actor_id = actor.json()['result']['id']\n",
    "\n",
    "\n",
    "# Executions of this actor, lots of times.\n",
    "exec_id_list = []\n",
    "for i in range(executions):\n",
    "    execution = r.post(f\"{url}/actors/{actor_id}/messages\",\n",
    "                       headers=header_dat,\n",
    "                       data={'message':messageDat})\n",
    "    exec_id = execution.json()['result']['executionId']\n",
    "    exec_id_list.append(exec_id)\n",
    "\n",
    "\n",
    "# Getting result of exec from log once exec complete.\n",
    "results_dict = {}\n",
    "while exec_id_list:\n",
    "    for exec_id in exec_id_list:\n",
    "        exec_logs = r.get(f\"{url}/actors/{actor_id}/executions/{exec_id}/logs\",\n",
    "                          headers=header_dat)\n",
    "\n",
    "        logs = exec_logs.json()['result']['logs']\n",
    "        if logs:\n",
    "            results_dict[f'id - {exec_id}'] = float(logs.replace('\\n',''))\n",
    "            exec_id_list.remove(exec_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = \"'threads' 'std_deviation' 'size'\"\n",
    "messageDat = '1 10 1500'\n",
    "executions = 8\n",
    "\n",
    "msg_start = time.time()\n",
    "exec_id_list = []\n",
    "for i in range(executions):\n",
    "    execution = r.post(f\"{url}/actors/{actor_id}/messages\",\n",
    "                       headers=header_dat,\n",
    "                       data={'message':messageDat})\n",
    "    exec_id = execution.json()['result']['executionId']\n",
    "    exec_id_list.append(exec_id)\n",
    "msg_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_start = time.time()\n",
    "results_list = []\n",
    "while exec_id_list:\n",
    "    for exec_id in exec_id_list:\n",
    "        exec_logs = r.get(f\"{url}/actors/{actor_id}/executions/{exec_id}/logs\",\n",
    "                          headers=header_dat)\n",
    "\n",
    "        logs = exec_logs.json()['result']['logs']\n",
    "        if logs:\n",
    "            print('. ', end='')\n",
    "            results_list.append([f'id - {exec_id}', float(logs.replace('\\n',''))])\n",
    "            exec_id_list.remove(exec_id)\n",
    "exec_end = time.time()\n",
    "\n",
    "work_time = 0\n",
    "for res in results_list:\n",
    "    work_time += res[1]\n",
    "\n",
    "print(f\"\\n\\nExec Time: {exec_end - exec_start}\")\n",
    "print(f\"Message Time: {msg_end - msg_start}\")\n",
    "print(f\"Work Time: {work_time}\")\n",
    "pp.pprint(results_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
