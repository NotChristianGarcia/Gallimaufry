{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "import redis\n",
    "import json\n",
    "import copy\n",
    "import datetime\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client.abaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_to_dt(unix):\n",
    "    return datetime.datetime.utcfromtimestamp(float(unix))\n",
    "\n",
    "def docker_time_to_dt(docker_time):\n",
    "    return datetime.datetime.strptime(docker_time.replace('Z', '')[:-1], \"%Y-%m-%dT%H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mongo Conversion\n",
    "# Helper functions that converts Mongo to new formatting\n",
    "def convertLogs(base_list, executions):\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "    newList = []\n",
    "    for actor in copied_list:\n",
    "        aid = actor['_id']\n",
    "        newDict = {'exp': actor['exp'], '_id': aid, 'logs': actor[aid]}\n",
    "        newList.append(newDict)\n",
    "        \n",
    "    for log in newList:\n",
    "        wanted_exec_id = log['_id']\n",
    "        for execution in executions:\n",
    "            if execution['id'] == wanted_exec_id:\n",
    "                log['actor_id'] = execution['actor_id']\n",
    "                log['tenant'] = execution['tenant']\n",
    "                break\n",
    "    return newList\n",
    "\n",
    "def convertPermissions(base_list):\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "    newList = []\n",
    "    for actor in copied_list:\n",
    "        aid = actor['_id']\n",
    "        newDict = {'_id': aid}\n",
    "        newDict.update(actor[aid])\n",
    "        newList.append(newDict)\n",
    "    return newList\n",
    "\n",
    "def convertExecutions(base_list):\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "    newList = []\n",
    "    for actor_outer in copied_list:\n",
    "        del actor_outer['_id']\n",
    "        for aid, execution in actor_outer.items():\n",
    "            for exec_id, exec_dict in execution.items():\n",
    "                exec_dict['_id'] = f\"{aid}_{exec_id}\"\n",
    "                if exec_dict.get('start_time'):\n",
    "                    exec_dict['start_time'] = unix_to_dt(exec_dict['start_time'])\n",
    "                if exec_dict.get('message_received_time'):\n",
    "                    exec_dict['message_received_time'] = unix_to_dt(exec_dict['message_received_time'])\n",
    "                if exec_dict.get('final_state'):\n",
    "                    if exec_dict.get('final_state').get('StartedAt'):\n",
    "                        exec_dict['final_state']['StartedAt'] = docker_time_to_dt(exec_dict['final_state']['StartedAt'])\n",
    "                    if exec_dict.get('final_state').get('FinishedAt'):\n",
    "                        exec_dict['final_state']['FinishedAt'] = docker_time_to_dt(exec_dict['final_state']['FinishedAt'])\n",
    "                newList.append(exec_dict)\n",
    "    return newList\n",
    "\n",
    "def convertClients(base_list):\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "    newList = []\n",
    "    for actor in copied_list:\n",
    "        aid = actor['_id']\n",
    "        newDict = {'_id': aid}\n",
    "        newDict.update(actor[aid])\n",
    "        newList.append(newDict)\n",
    "    return newList\n",
    "\n",
    "\n",
    "# Redis Conversion\n",
    "# Helper functions that converts Redis to new formatting\n",
    "def convertActors(base_list):\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "    for actor_dict in copied_list:\n",
    "        if actor_dict.get('last_update_time'):\n",
    "            actor_dict['last_update_time'] = unix_to_dt(actor_dict['last_update_time'])\n",
    "        if actor_dict.get('create_time'):\n",
    "            actor_dict['create_time'] = unix_to_dt(actor_dict['create_time'])\n",
    "    return copied_list\n",
    "\n",
    "def convertWorkers(base_list):\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "    newList = []\n",
    "    for actor in copied_list:\n",
    "        aid = actor.pop('_id')\n",
    "        for worker_id, worker_dict in actor.items():\n",
    "            worker_dict['_id'] = f\"{aid}_{worker_id}\"\n",
    "            worker_dict['actor_id'] = aid\n",
    "            if worker_dict.get('create_time'):\n",
    "                worker_dict['create_time'] = unix_to_dt(worker_dict['create_time'])\n",
    "            if worker_dict.get('last_health_check_time'):\n",
    "                worker_dict['last_health_check_time'] = unix_to_dt(worker_dict['last_health_check_time'])\n",
    "            newList.append(worker_dict)\n",
    "    return newList\n",
    "\n",
    "def convertNonces(base_list):\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "    for nonce in copied_list:\n",
    "        for nonce_id, nonce_dict in nonce.items():\n",
    "            if not nonce_id == \"_id\":\n",
    "                if nonce_dict.get('last_use_time'):\n",
    "                    nonce_dict['last_use_time'] = unix_to_dt(nonce_dict['last_use_time'])\n",
    "                if nonce_dict.get('create_time'):\n",
    "                    nonce_dict['create_time'] = unix_to_dt(nonce_dict['create_time'])\n",
    "    return copied_list\n",
    "\n",
    "# Redis Reader\n",
    "# Helper function that reads a Redis DB into a JSON dict\n",
    "def redis2dict(db):\n",
    "    allDocs = []\n",
    "    redisDB = redis.Redis(db=db, port=6379)\n",
    "    for key in redisDB.scan_iter():\n",
    "        key = key.decode('utf-8')\n",
    "        jsonDict = json.loads(redisDB.get(key))\n",
    "        jsonDict['_id'] = key\n",
    "        allDocs.append(jsonDict)\n",
    "    return allDocs\n",
    "\n",
    "# Metrics database creation\n",
    "def createMetrics(base_list):\n",
    "    actor_dbids = []\n",
    "    actor_total = 0\n",
    "    execution_dbids = []\n",
    "    execution_total = 0\n",
    "\n",
    "    copied_list = copy.deepcopy(base_list)\n",
    "\n",
    "    for actor in copied_list:\n",
    "        actor_dbids.append(actor.pop('_id'))\n",
    "        actor_total += 1\n",
    "        for _, actor_inner in actor.items():\n",
    "            for _, execution in actor_inner.items():\n",
    "                execution_dbids.append(f'{execution[\"actor_id\"]}_{execution[\"id\"]}')\n",
    "                execution_total += 1\n",
    "                \n",
    "    metrics = [{'_id': 'stats',\n",
    "                'actor_total': actor_total,\n",
    "                'actor_dbids': actor_dbids,\n",
    "                'execution_total': execution_total,\n",
    "                'execution_dbids': execution_dbids}]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_logs_store_json = list(db['1'].find({}))\n",
    "base_permissions_store_json = list(db['2'].find({}))\n",
    "base_executions_store_json = list(db['3'].find({}))\n",
    "base_clients_store_json = list(db['4'].find({}))\n",
    "base_actors_store_json = redis2dict('1')\n",
    "base_workers_store_json = redis2dict('2')\n",
    "base_nonce_store_json = redis2dict('3')\n",
    "base_alias_store_json = redis2dict('4')\n",
    "base_pregen_clients_json = redis2dict('5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_permissions = convertPermissions(base_permissions_store_json)\n",
    "converted_executions = convertExecutions(base_executions_store_json)\n",
    "converted_clients = convertClients(base_clients_store_json)\n",
    "converted_actors = convertActors(base_actors_store_json)\n",
    "converted_workers = convertWorkers(base_workers_store_json)\n",
    "converted_nonces = convertNonces(base_nonce_store_json)\n",
    "converted_aliases = base_alias_store_json\n",
    "converted_pregen = base_pregen_clients_json\n",
    "converted_logs = convertLogs(base_logs_store_json, converted_executions)\n",
    "new_metrics = createMetrics(base_executions_store_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_naming = {'n1': converted_logs,\n",
    "             'n2': converted_permissions,\n",
    "             'n3': converted_executions,\n",
    "             'n4': converted_clients,\n",
    "             'n5': converted_actors,\n",
    "             'n6': converted_workers,\n",
    "             'n7': converted_nonces,\n",
    "             'n8': converted_aliases,\n",
    "             'n9': converted_pregen,\n",
    "             'n10': new_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing db: n1\n",
      "Currently processing db: n2\n",
      "Currently processing db: n3\n",
      "Currently processing db: n4\n",
      "Currently processing db: n5\n",
      "Currently processing db: n6\n",
      "Currently processing db: n7\n",
      "Currently processing db: n8\n",
      "Currently processing db: n9\n"
     ]
    }
   ],
   "source": [
    "# Convert and upload modified data\n",
    "for db_name, db_data in db_naming.items(): \n",
    "    try:\n",
    "        print(f\"Currently processing db: {db_name}\")\n",
    "        db[db_name].drop()\n",
    "        db[db_name].insert_many(db_data)\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Stuff Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def issueRaise(new, old, words):\n",
    "    print(f\"Issue - {words}\")\n",
    "    pprint(new)\n",
    "    print(\"=======================================\")\n",
    "    pprint(old)\n",
    "    raise ValueError(words)\n",
    "\n",
    "def increment(store, key):\n",
    "    if key in store:\n",
    "        store[key] += 1\n",
    "    else:\n",
    "        store[key] = 1\n",
    "\n",
    "def incrementVals(store, key, val):\n",
    "    if key in store:\n",
    "        store[key].append(val)\n",
    "    else:\n",
    "        store[key] = [val]\n",
    "        \n",
    "def lenAndValCheck(inpDict):\n",
    "    try:  \n",
    "        keyStore = {}\n",
    "        valStore = {}\n",
    "        for i, actor in enumerate(inpDict):\n",
    "                for worker in actor:\n",
    "                    if worker == '_id':\n",
    "                        increment(keyStore, '#actor_ids')\n",
    "                        incrementVals(valStore, '#actor_ids', actor['_id'])\n",
    "                    else:\n",
    "                        for key, val in actor[worker].items():\n",
    "                            increment(keyStore, key)\n",
    "                            incrementVals(valStore, key, val)\n",
    "    except AttributeError:\n",
    "        keyStore = {}\n",
    "        valStore = {}\n",
    "        for i, actor in enumerate(inpDict):\n",
    "                for key, val in actor.items():\n",
    "                    increment(keyStore, key)\n",
    "                    incrementVals(valStore, key, val)\n",
    "    return keyStore, valStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Convert old_coll to new stylings and use that as input.\n",
    "def conversionChecker(new, old, debug=False):\n",
    "    newKeys, newVals = lenAndValCheck(new)\n",
    "    oldKeys, oldVals = lenAndValCheck(old)\n",
    "    if debug:\n",
    "        print(f'New Keys:\\n{newKeys}\\n Old Keys:\\n{oldKeys}')\n",
    "    for key in newKeys:\n",
    "        try:\n",
    "            if not newKeys[key] == oldKeys[key]:\n",
    "                issueRaise(newVals[key], oldVals[key], f\"Value lengths are not the same. {newKeys[key]} and {oldKeys[key]}.\")\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Key: {key} not found in old JSON.\")\n",
    "    for key in oldKeys:\n",
    "        try:\n",
    "            if not newKeys[key] == oldKeys[key]:\n",
    "                issueRaise(newVals[key], oldVals[key], f\"Value lengths are not the same. {newKeys[key]} and {oldKeys[key]}.\")\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Key: '{key}' not found in new JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logs_store_json = list(db['1'].find({}))\n",
    "new_permissions_store_json = list(db['2'].find({}))\n",
    "new_executions_store_json = list(db['3'].find({}))\n",
    "new_clients_store_json = list(db['4'].find({}))\n",
    "new_actors_store_json = list(db['5'].find({}))\n",
    "new_workers_store_json = list(db['6'].find({}))\n",
    "new_nonce_store_json = list(db['7'].find({}))\n",
    "new_alias_store_json = list(db['8'].find({}))\n",
    "new_pregen_clients_json = list(db['9'].find({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get JSON for each Redis store\n",
    "old_logs_store_json = list(db['1'].find({}))\n",
    "old_permissions_store_json = list(db['2'].find({}))\n",
    "old_executions_store_json = list(db['3'].find({}))\n",
    "old_clients_store_json = list(db['4'].find({}))\n",
    "old_actors_store_json = redis2dict('1')\n",
    "old_workers_store_json = redis2dict('2')\n",
    "old_nonce_store_json = redis2dict('3')\n",
    "old_alias_store_json = redis2dict('4')\n",
    "old_pregen_clients_json = redis2dict('5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key: 'exp' not found in new JSON.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1789b2b0e0ad>\u001b[0m in \u001b[0;36mconversionChecker\u001b[0;34m(new, old, debug)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnewKeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moldKeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0missueRaise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewVals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moldVals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Value lengths are not the same. {newKeys[key]} and {oldKeys[key]}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'exp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-290-4f0c3587bb5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconversionChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_logs_store_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdateMongoLogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_logs_store_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-1789b2b0e0ad>\u001b[0m in \u001b[0;36mconversionChecker\u001b[0;34m(new, old, debug)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0missueRaise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewVals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moldVals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Value lengths are not the same. {newKeys[key]} and {oldKeys[key]}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key: '{key}' not found in new JSON.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: \"Key: 'exp' not found in new JSON.\""
     ]
    }
   ],
   "source": [
    "conversionChecker(new_logs_store_json, updateMongoLogs(old_logs_store_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversionChecker(new_permissions_store_json, updateMongoPermissions(old_permissions_store_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversionChecker(new_executions_store_json, updateMongoExecutions(old_executions_store_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversionChecker(new_actors_store_json, old_actors_store_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversionChecker(new_workers_store_json, old_workers_store_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversionChecker(new_nonce_store_json, old_nonce_store_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversionChecker(new_alias_store_json, old_alias_store_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversionChecker(new_pregen_clients_json, old_pregen_clients_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Convert old_coll to new stylings and use that as input.\n",
    "def conversionChecker(new, old):\n",
    "    if not type(new) == type(old):\n",
    "        issueRaise(new, old, \"Types aren't the same.\")\n",
    "    if isinstance(new, list) and isinstance(old, list):\n",
    "        if not len(new) == len(old):\n",
    "            issueRaise(new, old, \"Lists aren't the same length.\")\n",
    "        for new_iter, old_iter in zip(new, old):\n",
    "            conversionChecker(new_iter, old_iter)\n",
    "    elif isinstance(new, dict) and isinstance(old, dict):\n",
    "        if not len(new) == len(old):\n",
    "            issueRaise(new, old, \"Dict's aren't the same length.\")\n",
    "        if not new.keys() == old.keys():\n",
    "            for new_key, old_key in zip(new.keys(), old.keys()):\n",
    "                if not new_key == old_key:\n",
    "                    if not len(new_key) == len(old_key):\n",
    "                        issueRaise(new_key, old_key, \"Dict's keys aren't the same.\")\n",
    "\n",
    "        for new_key, old_key in zip(new.keys(), old.keys()):\n",
    "            if new_key in ['_id', 'exp', 'start_time', 'io', 'runtime', 'cpu', 'FinishedAt', 'StartedAt',\n",
    "                           'create_time', 'last_update_time', 'message_received_time']:\n",
    "                pass\n",
    "            elif new_key in ['actor_id', 'logs', 'id', 'worker_id', 'db_id']:\n",
    "                if not len(new[new_key]) == len(old[old_key]):\n",
    "                    issueRaise(new_key, old_key, f\"Dict's keys aren't the same. {new[new_key]} | {old[old_key]}\")\n",
    "            else:\n",
    "                conversionChecker(new[new_key], old[old_key])\n",
    "\n",
    "    else:\n",
    "        if not new == old:\n",
    "            issueRaise(new, old, \"Items aren't the same.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
